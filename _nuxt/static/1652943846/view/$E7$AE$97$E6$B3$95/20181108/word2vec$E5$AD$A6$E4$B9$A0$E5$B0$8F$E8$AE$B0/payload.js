__NUXT_JSONP__("/view/$E7$AE$97$E6$B3$95/20181108/word2vec$E5$AD$A6$E4$B9$A0$E5$B0$8F$E8$AE$B0", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K){return {data:[{siteConfig:{siteName:t,siteHost:"https:\u002F\u002Fcellargalaxy.github.io",basePath:"\u002Fblog-code\u002F",navs:[{text:"文章",url:"\u002Fblog-code\u002Fpage\u002F1\u002F"},{text:"归档",url:"\u002Fblog-code\u002Farchive\u002F0\u002F"},{text:"画画",url:"\u002Fblog-code\u002Fhtml\u002Fhua.html"},{text:"开源",url:"\u002Fblog-code\u002Fhttps:\u002Fgithub.com\u002Fcellargalaxy\u002Fblog-vue"}],pageSize:10,urlReplace:{"^/file/blog/code":"http:\u002F\u002F123.207.79.108\u002Ffile\u002Ffile\u002Fblog\u002Fcode"},backgroundImage:{duration:u,fade:1000,images:[{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F01\u002F25\u002FcofTzDQXitjeVZ6.jpg",description:v,type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F01\u002F25\u002FsfaRJ2lVeM3NDbE.jpg",description:v,type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F01\u002F26\u002FKybiTdftam5Su7x.jpg",description:"青春猪头-双葉理央",type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F09\u002F08\u002FxdX73nfs24qgOYk.jpg",description:"京吹-明日香,久美子",type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F02\u002F14\u002FMAiruNcEFW2HYtg.jpg",description:"终将成为你-七海灯子,小糸侑",type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F02\u002F05\u002FEojdAxTDJsFpbPw.jpg",description:"玉子市场-北白川玉子",type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F01\u002F31\u002FAXxwJDRS9fmN2uU.jpg",description:"fate_hf-弓道馆",type:f},{url:"https:\u002F\u002Fi.loli.net\u002F2020\u002F02\u002F01\u002FaHhVObpJus6dnM4.jpg",description:"fate_hf-樱花树",type:f}]}},homeConfig:{brandInterval:u,brands:[{imageUrl:"https:\u002F\u002Fi.loli.net\u002F2020\u002F01\u002F21\u002FmMEAnwY5XPC2pFb.jpg",title:"日常",texts:["我们所度过的每个平凡的日常，也许就是连续不断发生的奇迹。","日々、私たちが過ごしている日常は、実は奇跡の連続なのかもしれない。"]},{imageUrl:"https:\u002F\u002Fi.loli.net\u002F2020\u002F04\u002F19\u002FH1MmXb9xPcYEhT2.jpg",title:"昨日之歌",texts:["时间梭梭箭如飞，人道漫漫步蹒跚","人間そんな変わるもんじゃないのに、月日ばっかどんどん過ぎて"]}],navs:[{text:"Github",url:"https:\u002F\u002Fgithub.com\u002Fcellargalaxy\u002F"}]},pageFootConfig:{lines:[[{text:"Copyright © 2017-? ."},{text:"备案？不存在的"},{text:"Powered by Nuxt.js & Github"}]]},buildTime:new Date(1652943854671),siteName:t,file:{slug:l,description:m,createdAt:w,updatedAt:w,toc:[],body:{type:x,children:[{type:a,value:c},{type:b,tag:n,props:{id:y},children:[{type:b,tag:g,props:{href:"#%E8%AF%8D%E5%90%91%E9%87%8F",ariaHidden:o,tabIndex:p},children:[{type:b,tag:q,props:{className:[r,s]},children:[]}]},{type:a,value:y}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"在机器学习中，为了让计算机能够处理自然语言，我们需要用数字来表示自然语言中的词。如果使用一个向量来表示一个词，那这个向量就叫做词向量。把一个词转换为一个向量的方式有很多，例如"},{type:b,tag:d,props:{},children:[{type:a,value:z}]},{type:a,value:"，以及"},{type:b,tag:d,props:{},children:[{type:a,value:A}]},{type:a,value:"。"},{type:b,tag:d,props:{},children:[{type:a,value:z}]},{type:a,value:"比较简单，通过一个长度为词表大小的（高维）二进制向量来表示，但只能给词编个号，无法表示词之间的关系。另外一种叫做"},{type:b,tag:d,props:{},children:[{type:a,value:B}]},{type:a,value:"，使用稠密、低维的实数向量，以“具有相似上下文的词，应该具有相似的语义”的假说提起词义特征，能描述词之间的关系。而"},{type:b,tag:d,props:{},children:[{type:a,value:A}]},{type:a,value:"属于"},{type:b,tag:d,props:{},children:[{type:a,value:B}]},{type:a,value:"类型。"}]},{type:a,value:c},{type:b,tag:"div",props:{className:["nuxt-content-highlight"]},children:[{type:b,tag:"pre",props:{className:["language-text","line-numbers"]},children:[{type:b,tag:d,props:{},children:[{type:a,value:"\u002F\u002Fone hot Representation 向量维度=词表的大小\n'你' = [1 0 0 0 0 0 ......]\n'我' = [0 1 0 0 0 0 ......]\n'他' = [0 0 1 0 0 0 ......]\n\n\u002F\u002FDistributed Representation\n'你' = [0.54 0.68 0.12 ......]\n'我' = [0.14 0.69 0.81 ......]\n'他' = [0.17 0.93 0.52 ......]\n"}]}]}]},{type:a,value:c},{type:b,tag:n,props:{id:"cbow与skip-gram"},children:[{type:b,tag:g,props:{href:"#cbow%E4%B8%8Eskip-gram",ariaHidden:o,tabIndex:p},children:[{type:b,tag:q,props:{className:[r,s]},children:[]}]},{type:a,value:"CBOW与Skip-Gram"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"CBOW与Skip-Gram是两种识词模式。CBOW是通过上下文来推测中间的词，而Skip-Gram则是通过中间的词推测这个词的上下文。"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"我是上文 我是中间的词 我是下文"}]}]},{type:a,value:c},{type:b,tag:"ul",props:{},children:[{type:a,value:c},{type:b,tag:C,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"CBOW"}]},{type:a,value:D},{type:b,tag:d,props:{},children:[{type:a,value:E}]},{type:a,value:F},{type:b,tag:d,props:{},children:[{type:a,value:G}]}]},{type:a,value:c},{type:b,tag:C,props:{},children:[{type:b,tag:d,props:{},children:[{type:a,value:"Skip-Gram"}]},{type:a,value:D},{type:b,tag:d,props:{},children:[{type:a,value:G}]},{type:a,value:F},{type:b,tag:d,props:{},children:[{type:a,value:E}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:n,props:{id:H},children:[{type:b,tag:g,props:{href:"#%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%AD%E7%BB%83",ariaHidden:o,tabIndex:p},children:[{type:b,tag:q,props:{className:[r,s]},children:[]}]},{type:a,value:H}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:I,props:{alt:m,"data-src":"http:\u002F\u002F123.207.79.108\u002Ffile\u002Ffile\u002Fblog\u002Fcode\u002F20181108\u002Fpic4.zhimg.com-v2-a1a73c063b32036429fbd8f1ef59034b_r.jpg.1.jpg"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"如上图，词向量是通过神经网络来训练的。对于CBOW，把中间词"},{type:b,tag:d,props:{},children:[{type:a,value:"xk"}]},{type:a,value:"的one hot Representation作为输入，上下文"},{type:b,tag:d,props:{},children:[{type:a,value:"yj"}]},{type:a,value:"的one hot Representation作为输出，而Skip-Gram则相反。当神经网络训练完成之后，神经网络里的权重就作为词向量。其中，输入层到隐层的权重叫做输入向量，隐层到输出层的权重叫输出向量，一般使用输入向量。"}]},{type:a,value:c},{type:b,tag:"blockquote",props:{},children:[{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。\n此外，我们刚说了，输出 y 也是用 V 个节点表示的，对应V个词语，所以其实，我们把输出节点置成 [1,0,0,…,0]，它也能表示『吴彦祖』这个单词，但是激活的是隐含层到输出层的权重，这些权重的个数，跟隐含层一样，也可以组成一个向量 vy，跟上面提到的 vx 维度一样，并且可以看做是词语『吴彦祖』的另一种词向量。而这两种词向量 vx 和 vy，正是 Mikolov 在论文里所提到的，『输入向量』和『输出向量』，一般我们用『输入向量』。"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"上面是参考文章的片段，如他所说输入输出是one-hot encoder。但显然如果有几百万个词的话，输入输出的维度就会超级高。所以我看别的文章说，通过在输入使用word2vec来降低输入的维度，而最开始的word2vec是随机的。并且对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是简单的对所有输入词向量求和取平均。"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:I,props:{alt:m,"data-src":"http:\u002F\u002F123.207.79.108\u002Ffile\u002Ffile\u002Fblog\u002Fcode\u002F20181108\u002Fimages2017.cnblogs.com-blog-1042406-201707-1042406-20170727105752968-819608237.png.1.png"},children:[]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"而在输出层的优化上，会使用哈夫曼树，来避免判断全部词。哈夫曼树的叶子节点对应输出层，也就是词有多少个，叶子结点就有多少个。显然，我们需要从根节点一层一层往下走，走到正确的叶子节点那。而往左孩子还是又孩子走决定是通过一个逻辑回归的模型决定的。这个逻辑回归的模型的公式就不贴了，公式中包含当前内部节点的词向量（看不懂啥东西）以及一个每个节点都不一样的，用于调整的参数。最终使得时间复杂度减低为log。"}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:a,value:"参考文章："}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:g,props:{href:"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F26306795",rel:[h,i,j],target:k},children:[{type:a,value:"[NLP] 秒懂词向量Word2vec的本质"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:g,props:{href:"http:\u002F\u002Fwww.cnblogs.com\u002Fpinard\u002Fp\u002F7160330.html",rel:[h,i,j],target:k},children:[{type:a,value:"word2vec原理(一) CBOW与Skip-Gram模型基础"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:g,props:{href:"http:\u002F\u002Fwww.cnblogs.com\u002Fpinard\u002Fp\u002F7243513.html",rel:[h,i,j],target:k},children:[{type:a,value:"word2vec原理(二) 基于Hierarchical Softmax的模型"}]}]},{type:a,value:c},{type:b,tag:e,props:{},children:[{type:b,tag:g,props:{href:"https:\u002F\u002Fwww.jianshu.com\u002Fp\u002F418f27df3968",rel:[h,i,j],target:k},children:[{type:a,value:l}]}]}]},excerpt:{type:x,children:[]},dir:J,path:"\u002F算法\u002F20181108\u002Fword2vec学习小记",extension:".md",title:l,url:"\u002Fblog-code\u002Fview\u002F$E7$AE$97$E6$B3$95\u002F20181108\u002Fword2vec$E5$AD$A6$E4$B9$A0$E5$B0$8F$E8$AE$B0\u002F",createAt:new Date(1541635200000),updateAt:new Date(1541635200000),attributes:[{name:"createAt",value:K},{name:"updateAt",value:K},{name:"sort",value:J,url:"\u002Fblog-code\u002Fpage\u002F$E7$AE$97$E6$B3$95\u002F20181108\u002F1\u002F"}]}}],fetch:{},mutations:void 0}}("text","element","\n","code","p","wide","a","nofollow","noopener","noreferrer","_blank","word2vec学习小记","","h1","true",-1,"span","icon","icon-link","无名の窝",10000,"青春猪头-牧之原翔子-江之岛","2018-11-08T00:00:00.000Z","root","词向量","one hot Representation","word2vec","Distributed Representation","li","：","中间的词","→","上下文","词向量的训练","img","\u002F算法\u002F20181108","2018-11-08")));